import streamlit as st
import os
import json
import asyncio
from datetime import datetime
from pageindex import page_index_main, config
from pageindex.page_index_md import md_to_tree
from pageindex.utils import ConfigLoader, ChatGPT_API, ChatGPT_API_async, get_text_of_pages, remove_fields
import pandas as pd

st.set_page_config(page_title="PageIndex ç½‘é¡µç•Œé¢", page_icon="ğŸŒ²", layout="wide")

# Helper Functions
def update_api_config(api_key, api_base):
    os.environ["CHATGPT_API_KEY"] = api_key
    os.environ["CHATGPT_API_BASE"] = api_base
    import pageindex.utils
    pageindex.utils.CHATGPT_API_KEY = api_key
    pageindex.utils.CHATGPT_API_BASE = api_base

def get_file_size_str(size_bytes):
    """å°†å­—èŠ‚å¤§å°è½¬æ¢ä¸ºå¯è¯»æ ¼å¼"""
    if size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.2f} KB"
    else:
        return f"{size_bytes / (1024 * 1024):.2f} MB"

def get_file_type(filename):
    """è·å–æ–‡ä»¶ç±»å‹æè¿°"""
    ext = os.path.splitext(filename)[1].lower()
    type_map = {
        '.pdf': 'PDF æ–‡æ¡£',
        '.md': 'Markdown æ–‡æ¡£',
        '.markdown': 'Markdown æ–‡æ¡£'
    }
    return type_map.get(ext, 'æœªçŸ¥ç±»å‹')

def get_uploaded_files_info(upload_dir):
    """è·å–å·²ä¸Šä¼ æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯"""
    files_info = []
    if os.path.exists(upload_dir):
        for idx, filename in enumerate(os.listdir(upload_dir), 1):
            filepath = os.path.join(upload_dir, filename)
            if os.path.isfile(filepath):
                stat = os.stat(filepath)
                files_info.append({
                    'åºå·': idx,
                    'æ–‡ä»¶å': filename,
                    'ä¸Šä¼ æ—¶é—´': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                    'æ–‡ä»¶å¤§å°': get_file_size_str(stat.st_size),
                    'æ–‡ä»¶ç±»å‹': get_file_type(filename)
                })
    return files_info

def check_duplicate_files(uploaded_files, upload_dir):
    """æ£€æµ‹é‡å¤æ–‡ä»¶"""
    duplicates = []
    if os.path.exists(upload_dir):
        existing_files = set(os.listdir(upload_dir))
        for uploaded_file in uploaded_files:
            if uploaded_file.name in existing_files:
                duplicates.append(uploaded_file.name)
    return duplicates

async def select_relevant_docs(query, docs_info, model):
    """è®© LLM æ ¹æ®æ–‡æ¡£åç§°å’Œæè¿°é€‰æ‹©ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£ã€‚"""
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£è·¯ç”±ä»£ç†ã€‚ä½ æœ‰ä¸€ä»½åŒ…å«æ–‡æ¡£åç§°å’Œæè¿°çš„åˆ—è¡¨ã€‚
ç”¨æˆ·æœ‰ä¸€ä¸ªé—®é¢˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯é€‰æ‹©å¯èƒ½åŒ…å«ç­”æ¡ˆçš„èŠ‚ç‚¹ IDï¼ˆæ–‡æ¡£æ–‡ä»¶åï¼‰ã€‚

é—®é¢˜: {query}

æ–‡æ¡£åˆ—è¡¨:
{json.dumps(docs_info, indent=2, ensure_ascii=False)}

è¯·ä»…ä»¥ä»¥ä¸‹ JSON æ ¼å¼å›å¤:
{{
    "relevant_docs": ["filename1.json", "filename2.json"]
}}
å¦‚æœæ²¡æœ‰ç›¸å…³æ–‡æ¡£ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚"""
    response = await ChatGPT_API_async(model=model, prompt=prompt)
    try:
        content = response.strip()
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        return json.loads(content).get("relevant_docs", [])
    except Exception as e:
        st.error(f"æ–‡æ¡£ç­›é€‰è§£æå¤±è´¥: {e}")
        return []

async def tree_search(query, tree, model):
    # å‡†å¤‡ä¸åŒ…å«å®Œæ•´æ–‡æœ¬çš„æ ‘ç»“æ„ç”¨äºæ£€ç´¢
    tree_for_search = remove_fields(tree, fields=['text'])
    
    search_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£æ£€ç´¢ä¸“å®¶ã€‚ä½ å°†æ”¶åˆ°ä¸€ä¸ªç”¨æˆ·é—®é¢˜å’Œä¸€ä¸ªæ–‡æ¡£çš„å±‚çº§æ ‘ç»“æ„ã€‚
æ ‘ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰ `node_id`ã€`title` å’Œ `summary`ã€‚

ä½ çš„ç›®æ ‡æ˜¯è¯†åˆ«æœ€ç›¸å…³çš„èŠ‚ç‚¹ï¼Œè¿™äº›èŠ‚ç‚¹åŒ…å«å›ç­”é—®é¢˜æ‰€éœ€çš„ä¿¡æ¯ã€‚
- ä¼˜å…ˆé€‰æ‹©å¶å­èŠ‚ç‚¹ï¼ˆå±‚çº§åº•éƒ¨çš„èŠ‚ç‚¹ï¼‰ï¼Œå› ä¸ºå®ƒä»¬åŒ…å«å®é™…çš„é¡µé¢å†…å®¹ã€‚
- å¦‚æœä¿¡æ¯åˆ†å¸ƒåœ¨ä¸åŒéƒ¨åˆ†ï¼Œå¯ä»¥é€‰æ‹©å¤šä¸ªèŠ‚ç‚¹ã€‚
- åœ¨ `thinking` å­—æ®µä¸­æä¾›ä½ çš„æ¨ç†è¿‡ç¨‹ã€‚

é—®é¢˜: {query}

æ–‡æ¡£æ ‘ç»“æ„:
{json.dumps(tree_for_search, indent=2, ensure_ascii=False)}

è¯·ä»…ä»¥ä»¥ä¸‹ JSON æ ¼å¼å›å¤:
{{
    "thinking": "<é€æ­¥æ¨ç†ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›èŠ‚ç‚¹>",
    "node_list": ["node_id_1", "node_id_2", ...]
}}"""
    response = await ChatGPT_API_async(model=model, prompt=search_prompt)
    try:
        content = response.strip()
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
        return json.loads(content)
    except Exception as e:
        return {"thinking": f"è§£æå¤±è´¥: {e}", "node_list": []}

def get_node_mapping(structure, mapping=None):
    if mapping is None: mapping = {}
    if isinstance(structure, list):
        for item in structure:
            get_node_mapping(item, mapping)
    elif isinstance(structure, dict):
        if 'node_id' in structure:
            mapping[structure['node_id']] = structure
        if 'nodes' in structure:
            get_node_mapping(structure['nodes'], mapping)
    return mapping

# ä¾§è¾¹æ é…ç½®
st.sidebar.header("æ¨¡å‹é…ç½®")
api_key = st.sidebar.text_input("API å¯†é’¥", value=os.getenv("CHATGPT_API_KEY", ""), type="password")
api_base = st.sidebar.text_input("API åŸºç¡€åœ°å€", value=os.getenv("CHATGPT_API_BASE", "https://api.openai.com/v1"))

config_loader = ConfigLoader()
default_config = config_loader.load()
model_name = st.sidebar.text_input("æ¨¡å‹åç§°", value=default_config.model)

st.sidebar.header("PageIndex é…ç½®")
toc_check_pages = st.sidebar.number_input("ç›®å½•æ£€æŸ¥é¡µæ•°", value=default_config.toc_check_page_num)
max_pages_per_node = st.sidebar.number_input("æ¯èŠ‚ç‚¹æœ€å¤§é¡µæ•°", value=default_config.max_page_num_each_node)
max_tokens_per_node = st.sidebar.number_input("æ¯èŠ‚ç‚¹æœ€å¤§ä»¤ç‰Œæ•°", value=default_config.max_token_num_each_node)

# é»˜è®¤è®¾ç½®
if_add_doc_description = "no"
if_add_node_text = "no"

st.title("ğŸŒ² PageIndex æ™ºèƒ½æ–‡æ¡£ä»£ç†")

tab1, tab2 = st.tabs(["ğŸ“„ æ–‡æ¡£å¤„ç†", "ğŸ’¬ æ™ºèƒ½å¯¹è¯"])

upload_dir = "uploads"
results_dir = "results"
os.makedirs(upload_dir, exist_ok=True)
os.makedirs(results_dir, exist_ok=True)

# é€‰é¡¹å¡ 1: æ–‡æ¡£å¤„ç†
with tab1:
    st.header("å¤„ç†æ–°æ–‡æ¡£")
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡ä»¶", 
        type=["pdf", "md", "markdown"], 
        accept_multiple_files=True,
        key="file_uploader"
    )

    # é‡å¤æ–‡ä»¶æ£€æµ‹
    if uploaded_files:
        duplicates = check_duplicate_files(uploaded_files, upload_dir)
        if duplicates:
            st.warning(f"âš ï¸ æ£€æµ‹åˆ°é‡å¤æ–‡ä»¶ï¼ä»¥ä¸‹æ–‡ä»¶å·²å­˜åœ¨äºä¸Šä¼ ç›®å½•ä¸­ï¼š\n\n**{', '.join(duplicates)}**\n\nç»§ç»­å¤„ç†å°†è¦†ç›–åŸæœ‰æ–‡ä»¶ã€‚")

        if st.button("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†"):
            if not api_key:
                st.error("è¯·è¾“å…¥ API å¯†é’¥ï¼")
            else:
                update_api_config(api_key, api_base)
                total_files = len(uploaded_files)
                progress_bar = st.progress(0.0)
                status_text = st.empty()
                all_results_container = st.container()
                
                for i, uploaded_file in enumerate(uploaded_files):
                    file_extension = os.path.splitext(uploaded_file.name)[1].lower()
                    status_text.text(f"æ­£åœ¨å¤„ç† ({i+1}/{total_files}): {uploaded_file.name}...")
                    
                    try:
                        file_path = os.path.join(upload_dir, uploaded_file.name)
                        with open(file_path, "wb") as f:
                            f.write(uploaded_file.getvalue())
                        
                        result = None
                        if file_extension == ".pdf":
                            opt = config(
                                model=model_name,
                                toc_check_page_num=toc_check_pages,
                                max_page_num_each_node=max_pages_per_node,
                                max_token_num_each_node=max_tokens_per_node,
                                if_add_node_id="yes",
                                if_add_node_summary="yes",
                                if_add_doc_description=if_add_doc_description,
                                if_add_node_text=if_add_node_text
                            )
                            result = page_index_main(file_path, opt)
                        elif file_extension in [".md", ".markdown"]:
                            result = asyncio.run(md_to_tree(
                                md_path=file_path,
                                if_thinning=False,
                                if_add_node_summary=True,
                                model=model_name,
                                if_add_doc_description=(if_add_doc_description == "yes"),
                                if_add_node_text=True,  # Markdown æ–‡ä»¶å¼ºåˆ¶ä¿ç•™å®Œæ•´æ–‡æœ¬ä»¥æ”¯æŒæ£€ç´¢
                                if_add_node_id=True
                            ))

                        if result:
                            file_base_name = os.path.splitext(uploaded_file.name)[0]
                            result_file_path = os.path.join(results_dir, f"{file_base_name}_structure.json")
                            with open(result_file_path, "w", encoding="utf-8") as f:
                                json.dump(result, f, indent=2, ensure_ascii=False)
                            
                            with all_results_container:
                                with st.expander(f"âœ… {uploaded_file.name} å¤„ç†æˆåŠŸ", expanded=False):
                                    st.info(f"JSON å·²è‡ªåŠ¨ä¿å­˜è‡³: {result_file_path}")
                                    st.json(result)
                    except Exception as e:
                        with all_results_container:
                            st.error(f"âŒ {uploaded_file.name} å¤„ç†å‡ºé”™: {str(e)}")
                    progress_bar.progress((i + 1) / total_files)
                status_text.text("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼")
                st.balloons()

    # æ–‡ä»¶è¯¦ç»†æ¸…å•
    st.markdown("---")
    files_info = get_uploaded_files_info(upload_dir)
    
    # æ ‡é¢˜å’Œåˆ é™¤æŒ‰é’®åœ¨åŒä¸€è¡Œ
    col_title, col_btn = st.columns([4, 1])
    with col_title:
        st.subheader("ğŸ“‹ å·²å¤„ç†æ–‡ä»¶æ¸…å•")
    
    if files_info:
        file_names = [f['æ–‡ä»¶å'] for f in files_info]
        
        # åˆå§‹åŒ–é€‰ä¸­çŠ¶æ€
        if "selected_files" not in st.session_state:
            st.session_state.selected_files = {name: False for name in file_names}
        
        # åŒæ­¥æ–°æ–‡ä»¶åˆ°é€‰ä¸­çŠ¶æ€
        for name in file_names:
            if name not in st.session_state.selected_files:
                st.session_state.selected_files[name] = False
        
        with col_btn:
            if st.button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­", type="secondary"):
                deleted_files = []
                for filename, selected in st.session_state.selected_files.items():
                    if selected:
                        # åˆ é™¤åŸå§‹æ–‡ä»¶
                        file_path = os.path.join(upload_dir, filename)
                        if os.path.exists(file_path):
                            os.remove(file_path)
                        
                        # åˆ é™¤å¯¹åº”çš„ç´¢å¼• JSON æ–‡ä»¶
                        file_base_name = os.path.splitext(filename)[0]
                        json_path = os.path.join(results_dir, f"{file_base_name}_structure.json")
                        if os.path.exists(json_path):
                            os.remove(json_path)
                        
                        deleted_files.append(filename)
                
                if deleted_files:
                    st.success(f"å·²åˆ é™¤ {len(deleted_files)} ä¸ªæ–‡ä»¶")
                    st.session_state.selected_files = {}
                    st.rerun()
                else:
                    st.warning("è¯·å…ˆé€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶")
        
        # ä½¿ç”¨ data_editor å®ç°ç´§å‡‘çš„å¯é€‰æ‹©è¡¨æ ¼
        df = pd.DataFrame(files_info)
        df.insert(0, 'é€‰æ‹©', False)
        
        # å°†åºå·è½¬ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿å±…ä¸­æ˜¾ç¤º
        df['åºå·'] = df['åºå·'].astype(str)
        
        edited_df = st.data_editor(
            df,
            column_config={
                "é€‰æ‹©": st.column_config.CheckboxColumn(
                    "é€‰æ‹©",
                    help="é€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶",
                    default=False,
                ),
                "åºå·": st.column_config.TextColumn("åºå·", width="small"),
                "æ–‡ä»¶å": st.column_config.TextColumn("æ–‡ä»¶å", width="medium"),
                "ä¸Šä¼ æ—¶é—´": st.column_config.TextColumn("ä¸Šä¼ æ—¶é—´", width="medium"),
                "æ–‡ä»¶å¤§å°": st.column_config.TextColumn("å¤§å°", width="small"),
                "æ–‡ä»¶ç±»å‹": st.column_config.TextColumn("ç±»å‹", width="small"),
            },
            disabled=["åºå·", "æ–‡ä»¶å", "ä¸Šä¼ æ—¶é—´", "æ–‡ä»¶å¤§å°", "æ–‡ä»¶ç±»å‹"],
            hide_index=True,
            use_container_width=True,
            key="file_table"
        )
        
        # æ›´æ–°é€‰ä¸­çŠ¶æ€
        for idx, row in edited_df.iterrows():
            st.session_state.selected_files[row['æ–‡ä»¶å']] = row['é€‰æ‹©']
        
        st.caption(f"å…± {len(files_info)} ä¸ªæ–‡ä»¶")
    else:
        with col_btn:
            st.empty()
        st.info("æš‚æ— å·²ä¸Šä¼ çš„æ–‡ä»¶ã€‚è¯·ä¸Šä¼ æ–‡ä»¶è¿›è¡Œå¤„ç†ã€‚")

# é€‰é¡¹å¡ 2: æ™ºèƒ½å¯¹è¯ (RAG)
with tab2:
    st.header("è·¨æ–‡æ¡£æ™ºèƒ½å¯¹è¯")
    
    # åŠ è½½æ‰€æœ‰å¯ç”¨ç´¢å¼•
    available_indices = [f for f in os.listdir(results_dir) if f.endswith("_structure.json")]
    
    if not available_indices:
        st.warning("å°šæœªå¤„ç†ä»»ä½•æ–‡æ¡£ã€‚è¯·å…ˆåœ¨ã€Œæ–‡æ¡£å¤„ç†ã€é€‰é¡¹å¡ä¸­å¤„ç†æ–‡ä»¶ã€‚")
    else:

        # åˆå§‹åŒ–èŠå¤©å†å²
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # æ˜¾ç¤ºèŠå¤©æ¶ˆæ¯
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                if "thinking" in message and message["thinking"]:
                    with st.expander("æ¨ç†æ£€ç´¢è¿‡ç¨‹"):
                        st.markdown(message["thinking"])
                if "nodes" in message and message["nodes"]:
                    with st.expander("å‚è€ƒæ¥æº"):
                        for node_info in message["nodes"]:
                            st.write(node_info)

        # èŠå¤©è¾“å…¥
        if query := st.chat_input("å‘æ•´ä¸ªæ–‡æ¡£åº“æé—®..."):
            if not api_key:
                st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® API å¯†é’¥")
            else:
                update_api_config(api_key, api_base)
                st.session_state.messages.append({"role": "user", "content": query})
                with st.chat_message("user"):
                    st.markdown(query)

                with st.chat_message("assistant"):
                    with st.status("æ­£åœ¨è¿›è¡Œå¤šæ–‡æ¡£æ™ºèƒ½æ£€ç´¢...", expanded=True) as status:
                        # 1. ç­›é€‰ç›¸å…³æ–‡æ¡£
                        st.write("1. ç­›é€‰ç›¸å…³æ–‡æ¡£...")
                        docs_info = []
                        for idx_file in available_indices:
                            with open(os.path.join(results_dir, idx_file), "r", encoding="utf-8") as f:
                                data = json.load(f)
                                docs_info.append({
                                    "filename": idx_file,
                                    "doc_name": data.get("doc_name", idx_file),
                                    "description": data.get("description", "æ— æè¿°")
                                })
                        
                        relevant_filenames = asyncio.run(select_relevant_docs(query, docs_info, model_name))
                        st.write(f"å·²ç­›é€‰å‡º {len(relevant_filenames)} ä¸ªç›¸å…³æ–‡æ¡£: {relevant_filenames}")
                        
                        if not relevant_filenames:
                            if len(available_indices) <= 3:
                                relevant_filenames = available_indices
                            else:
                                st.warning("æ¨¡å‹è®¤ä¸ºæ²¡æœ‰æ–‡æ¡£ä¸æ­¤é—®é¢˜ç›´æ¥ç›¸å…³ã€‚")
                                relevant_filenames = []
                        
                        # 2. åœ¨æ¯ä¸ªç›¸å…³æ–‡æ¡£ä¸­æœç´¢
                        all_relevant_text = ""
                        all_reference_nodes = []
                        total_thinking = ""
                        
                        for idx_file in relevant_filenames:
                            idx_path = os.path.join(results_dir, idx_file)
                            if not os.path.exists(idx_path): continue
                            
                            with open(idx_path, "r", encoding="utf-8") as f:
                                index_data = json.load(f)
                            
                            doc_display_name = index_data.get('doc_name', idx_file)
                            st.write(f"æ­£åœ¨æ£€ç´¢æ–‡æ¡£: {doc_display_name}...")
                            
                            # å¯¹æ­¤æ–‡æ¡£è¿›è¡Œæ ‘æœç´¢
                            search_res = asyncio.run(tree_search(query, index_data['structure'], model_name))
                            if search_res.get('thinking'):
                                total_thinking += f"**[{doc_display_name}]**: {search_res['thinking']}"
                            
                            node_map = get_node_mapping(index_data['structure'])
                            
                            pdf_name = index_data.get('doc_name', idx_file.replace("_structure.json", ""))
                            pdf_path = os.path.join(upload_dir, pdf_name)
                            if not os.path.exists(pdf_path):
                                for ext in [".pdf", ".md", ".markdown"]:
                                    if os.path.exists(pdf_path + ext):
                                        pdf_path = pdf_path + ext
                                        break

                            for node_id in search_res.get('node_list', []):
                                if node_id in node_map:
                                    node = node_map[node_id]
                                    title = node.get('title', 'æœªçŸ¥')
                                    start_p = node.get('start_index', '?')
                                    all_reference_nodes.append(f"[{doc_display_name}] {title} (ç¬¬{start_p}é¡µ)")
                                    
                                    if node.get('text'):
                                        all_relevant_text += f"--- æ–‡æ¡£: {doc_display_name}, ç« èŠ‚: {title} ---{node['text']}"
                                    elif os.path.exists(pdf_path) and pdf_path.lower().endswith(".pdf"):
                                        try:
                                            page_text = get_text_of_pages(pdf_path, node['start_index'], node['end_index'], tag=False)
                                            all_relevant_text += f"--- æ–‡æ¡£: {doc_display_name}, ç« èŠ‚: {title} ---{page_text}"
                                        except Exception as e:
                                            pass
                        
                        st.write("3. æ•´åˆçŸ¥è¯†ç”Ÿæˆå›ç­”...")
                        status.update(label="å¤šæ–‡æ¡£æ£€ç´¢å®Œæˆ", state="complete", expanded=False)

                    # 3. æœ€ç»ˆç­”æ¡ˆç”Ÿæˆ
                    if not all_relevant_text:
                        full_answer = "æŠ±æ­‰ï¼Œæ£€ç´¢è¿‡ç¨‹æœªèƒ½ä»ç›¸å…³æ–‡æ¡£ä¸­æå–åˆ°è¶³å¤Ÿçš„åŸæ–‡å†…å®¹ã€‚è¯·ç¡®ä¿æ–‡æ¡£å·²æ­£ç¡®å¤„ç†ä¸”æ–‡ä»¶æœªè¢«ç§»åŠ¨ã€‚"
                    else:
                        answer_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ã€‚ä½ æœ‰æ¥è‡ªå¤šä¸ªæ¥æºçš„æ–‡æ¡£ç‰‡æ®µã€‚
æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœæ¥æºæœ‰å†²çªçš„ä¿¡æ¯ï¼Œè¯·æåŠã€‚
åœ¨å›ç­”ä¸­å§‹ç»ˆå¼•ç”¨æ–‡æ¡£åç§°ã€‚

é—®é¢˜: {query}

ä¸Šä¸‹æ–‡:
{all_relevant_text[:15000]}

åŠ©æ‰‹:"""
                        full_answer = ChatGPT_API(model=model_name, prompt=answer_prompt)
                    
                    st.markdown(full_answer)
                    if all_reference_nodes:
                        with st.expander("å‚è€ƒæ¥æº"):
                            for node_info in all_reference_nodes:
                                st.write(node_info)
                    
                    # ä¿å­˜å†å²è®°å½•
                    st.session_state.messages.append({
                        "role": "assistant", 
                        "content": full_answer,
                        "thinking": total_thinking,
                        "nodes": all_reference_nodes
                    })

st.markdown("---")
st.caption("ç”± PageIndex æ¡†æ¶é©±åŠ¨ - æ— å‘é‡æ¨ç† RAG")
