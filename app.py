import streamlit as st
import os
import json
import asyncio
from datetime import datetime
from pageindex import page_index_main, config
from pageindex.page_index_md import md_to_tree
from pageindex.utils import ConfigLoader, ChatGPT_API, ChatGPT_API_async, get_text_of_pages, remove_fields
from pageindex.vector_index import get_vector_index, search_documents, build_index_for_document
import pandas as pd

st.set_page_config(page_title="PageIndex ç½‘é¡µç•Œé¢", page_icon="ğŸŒ²", layout="wide")

# å‡å°‘é¡µé¢é¡¶éƒ¨ç©ºç™½ï¼Œå¹¶è®¾ç½®ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨ä¸ºæ»šåŠ¨æ˜¾ç¤º
st.markdown("""
<style>
    .block-container {
        padding-top: 1rem;
        padding-bottom: 0rem;
    }
    header {
        visibility: hidden;
    }
    .stMainBlockContainer {
        padding-top: 1rem;
    }
    /* ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨æ»šåŠ¨æ˜¾ç¤º */
    [data-testid="stFileUploaderDropzoneInput"] + div {
        max-height: 200px;
        overflow-y: auto;
    }
    /* ä¸Šä¼ æ–‡ä»¶é¢„è§ˆåŒºåŸŸæ»šåŠ¨ */
    .stFileUploader > div > div:last-child {
        max-height: 150px;
        overflow-y: auto;
    }
</style>
""", unsafe_allow_html=True)

# Helper Functions
def update_api_config(api_key, api_base):
    os.environ["CHATGPT_API_KEY"] = api_key
    os.environ["CHATGPT_API_BASE"] = api_base
    import pageindex.utils
    pageindex.utils.CHATGPT_API_KEY = api_key
    pageindex.utils.CHATGPT_API_BASE = api_base

def get_file_size_str(size_bytes):
    """å°†å­—èŠ‚å¤§å°è½¬æ¢ä¸ºå¯è¯»æ ¼å¼"""
    if size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.2f} KB"
    else:
        return f"{size_bytes / (1024 * 1024):.2f} MB"

def get_file_type(filename):
    """è·å–æ–‡ä»¶ç±»å‹æè¿°"""
    ext = os.path.splitext(filename)[1].lower()
    type_map = {
        '.pdf': 'PDF æ–‡æ¡£',
        '.md': 'Markdown æ–‡æ¡£',
        '.markdown': 'Markdown æ–‡æ¡£'
    }
    return type_map.get(ext, 'æœªçŸ¥ç±»å‹')

def get_uploaded_files_info(upload_dir):
    """è·å–å·²ä¸Šä¼ æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯"""
    files_info = []
    if os.path.exists(upload_dir):
        for idx, filename in enumerate(os.listdir(upload_dir), 1):
            filepath = os.path.join(upload_dir, filename)
            if os.path.isfile(filepath):
                stat = os.stat(filepath)
                files_info.append({
                    'åºå·': idx,
                    'æ–‡ä»¶å': filename,
                    'ä¸Šä¼ æ—¶é—´': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                    'æ–‡ä»¶å¤§å°': get_file_size_str(stat.st_size),
                    'æ–‡ä»¶ç±»å‹': get_file_type(filename)
                })
    return files_info

def check_duplicate_files(uploaded_files, upload_dir):
    """æ£€æµ‹é‡å¤æ–‡ä»¶"""
    duplicates = []
    if os.path.exists(upload_dir):
        existing_files = set(os.listdir(upload_dir))
        for uploaded_file in uploaded_files:
            if uploaded_file.name in existing_files:
                duplicates.append(uploaded_file.name)
    return duplicates


def get_node_mapping(structure, mapping=None):
    """ä»æ ‘ç»“æ„ä¸­æ„å»º node_id åˆ°èŠ‚ç‚¹çš„æ˜ å°„"""
    if mapping is None: 
        mapping = {}
    if isinstance(structure, list):
        for item in structure:
            get_node_mapping(item, mapping)
    elif isinstance(structure, dict):
        if 'node_id' in structure:
            mapping[structure['node_id']] = structure
        if 'nodes' in structure:
            get_node_mapping(structure['nodes'], mapping)
    return mapping


def load_document_structure(doc_name: str, results_dir: str):
    """åŠ è½½æ–‡æ¡£çš„ç»“æ„ JSON æ–‡ä»¶"""
    possible_names = [
        f"{doc_name}_structure.json",
        f"{doc_name.replace('.pdf', '')}_structure.json",
        f"{doc_name.replace('.md', '')}_structure.json",
    ]
    
    for name in possible_names:
        path = os.path.join(results_dir, name)
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
    return None


# ä¾§è¾¹æ é…ç½®
st.sidebar.header("æ¨¡å‹é…ç½®")
api_key = st.sidebar.text_input("API å¯†é’¥", value=os.getenv("CHATGPT_API_KEY", ""), type="password")
api_base = st.sidebar.text_input("API åŸºç¡€åœ°å€", value=os.getenv("CHATGPT_API_BASE", "https://api.openai.com/v1"))

config_loader = ConfigLoader()
default_config = config_loader.load()
model_name = st.sidebar.text_input("æ¨¡å‹åç§°", value=os.getenv("CHATGPT_MODEL", "gpt-4o"))

st.sidebar.header("PageIndex é…ç½®")
toc_check_pages = st.sidebar.number_input("ç›®å½•æ£€æŸ¥é¡µæ•°", value=default_config.toc_check_page_num)
max_pages_per_node = st.sidebar.number_input("æ¯èŠ‚ç‚¹æœ€å¤§é¡µæ•°", value=default_config.max_page_num_each_node)
max_tokens_per_node = st.sidebar.number_input("æ¯èŠ‚ç‚¹æœ€å¤§ä»¤ç‰Œæ•°", value=default_config.max_token_num_each_node)

st.sidebar.header("å‘é‡æ£€ç´¢é…ç½®")
vector_top_k = st.sidebar.slider("æ£€ç´¢ç»“æœæ•°é‡ (Top-K)", min_value=1, max_value=50, value=10)

# é»˜è®¤è®¾ç½®
if_add_doc_description = "no"
if_add_node_text = "no"

# åŸç†ä»‹ç»å†…å®¹
PRINCIPLE_CONTENT = """
## PageIndex æ£€ç´¢åŸç†å¯¹æ¯”

### ä¸€ã€ä¸‰ç§æ£€ç´¢æ–¹å¼å¯¹æ¯”
| ç‰¹æ€§ | ä¼ ç»Ÿå‘é‡æ£€ç´¢ | PageIndex åŸç‰ˆï¼ˆVectorlessï¼‰ | æœ¬æ¬¡ä¼˜åŒ–ï¼ˆæ··åˆæ£€ç´¢ï¼‰ |
|------|-------------|---------------------------|-------------------|
| ç´¢å¼•æ–¹å¼ | æ–‡æ¡£åˆ‡ç‰‡ â†’ å‘é‡åŒ– | æ–‡æ¡£ â†’ ç›®å½•æ ‘ç»“æ„ | ç›®å½•æ ‘ + èŠ‚ç‚¹å‘é‡ |
| æ£€ç´¢æ–¹å¼ | å‘é‡ç›¸ä¼¼åº¦åŒ¹é… | LLM æ¨ç†å®šä½ | å‘é‡å¬å› + ç»“æ„å®šä½ |
| Token æ¶ˆè€— | 0ï¼ˆæ£€ç´¢é˜¶æ®µï¼‰ | é«˜ï¼ˆæ¯æ¬¡æŸ¥è¯¢éœ€å¤šæ¬¡ LLM è°ƒç”¨ï¼‰ | 0ï¼ˆæ£€ç´¢é˜¶æ®µï¼‰ |
| æ£€ç´¢é€Ÿåº¦ | æ¯«ç§’çº§ | ç§’çº§ï¼ˆä¾èµ– LLM å“åº”ï¼‰ | æ¯«ç§’çº§ |
| ä¸Šä¸‹æ–‡ä¿ç•™ | å·®ï¼ˆåˆ‡ç‰‡ç ´åä¸Šä¸‹æ–‡ï¼‰ | ä¼˜ï¼ˆä¿ç•™æ–‡æ¡£å±‚çº§ç»“æ„ï¼‰ | ä¼˜ï¼ˆä¿ç•™ç»“æ„ä¿¡æ¯ï¼‰ |
| è¯­ä¹‰ç†è§£ | æµ…å±‚ï¼ˆå‘é‡ç›¸ä¼¼åº¦ï¼‰ | æ·±å±‚ï¼ˆLLM æ¨ç†ï¼‰ | ä¸­ç­‰ï¼ˆå‘é‡ + æ‘˜è¦ï¼‰ |

### äºŒã€PageIndex åŸç‰ˆæ£€ç´¢åŸç†ï¼ˆVectorless RAGï¼‰
**æ ¸å¿ƒç†å¿µ**ï¼šä¸å¯¹æ–‡æ¡£è¿›è¡Œå‘é‡åŒ–ï¼Œè€Œæ˜¯åˆ©ç”¨ LLM çš„æ¨ç†èƒ½åŠ›åœ¨æ–‡æ¡£ç›®å½•ç»“æ„ä¸­å®šä½ä¿¡æ¯ã€‚

**æ£€ç´¢æµç¨‹**ï¼š
```
ç”¨æˆ·æŸ¥è¯¢ 
    â†“
1. LLM ç­›é€‰ç›¸å…³æ–‡æ¡£ï¼ˆæ ¹æ®æ–‡æ¡£åç§°å’Œæè¿°ï¼‰
    â†“
2. LLM åœ¨ç›®å½•æ ‘ä¸­æ¨ç†å®šä½ï¼ˆä¼ é€’å®Œæ•´æ ‘ç»“æ„ç»™ LLMï¼‰
    â†“
3. æ ¹æ®å®šä½çš„èŠ‚ç‚¹æå–åŸæ–‡ï¼ˆstart_index â†’ end_indexï¼‰
    â†“
4. LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
```

**ä¼˜ç‚¹**ï¼š
- ä¿ç•™æ–‡æ¡£çš„å±‚çº§ç»“æ„å’Œä¸Šä¸‹æ–‡å…³ç³»
- LLM å¯ä»¥è¿›è¡Œæ·±å±‚è¯­ä¹‰æ¨ç†
- ä¸éœ€è¦å‘é‡æ•°æ®åº“

**ç¼ºç‚¹**ï¼š
- æ¯æ¬¡æŸ¥è¯¢æ¶ˆè€—å¤§é‡ Tokenï¼ˆéœ€è¦ä¼ é€’å®Œæ•´æ ‘ç»“æ„ï¼‰  
- æ£€ç´¢é€Ÿåº¦æ…¢ï¼ˆä¾èµ– LLM å“åº”æ—¶é—´ï¼‰
- æ–‡æ¡£æ•°é‡å¢å¤šæ—¶ï¼ŒToken æ¶ˆè€—çº¿æ€§å¢é•¿

### ä¸‰ã€ä¼ ç»Ÿå‘é‡æ£€ç´¢åŸç†
**æ ¸å¿ƒç†å¿µ**ï¼šå°†æ–‡æ¡£åˆ‡åˆ†ä¸ºå›ºå®šå¤§å°çš„å—ï¼Œå‘é‡åŒ–åé€šè¿‡ç›¸ä¼¼åº¦åŒ¹é…æ£€ç´¢ã€‚

**æ£€ç´¢æµç¨‹**ï¼š
```
ç”¨æˆ·æŸ¥è¯¢ 
    â†“
1. æŸ¥è¯¢æ–‡æœ¬å‘é‡åŒ–
    â†“
2. å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ Top-K æ–‡æ¡£å—
    â†“
3. æ‹¼æ¥æ£€ç´¢åˆ°çš„æ–‡æ¡£å—
    â†“
4. LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
```

**ä¼˜ç‚¹**ï¼š
- æ£€ç´¢é€Ÿåº¦å¿«ï¼ˆæ¯«ç§’çº§ï¼‰
- æ£€ç´¢é˜¶æ®µä¸æ¶ˆè€— Token

**ç¼ºç‚¹**ï¼š
- åˆ‡ç‰‡ç ´åæ–‡æ¡£ä¸Šä¸‹æ–‡
- æ— æ³•ç†è§£æ–‡æ¡£å±‚çº§ç»“æ„
- å¯èƒ½æ£€ç´¢åˆ°ä¸ç›¸å…³çš„ç‰‡æ®µ

### å››ã€æœ¬ç³»ç»Ÿä¼˜åŒ–æ–¹æ¡ˆï¼ˆæ··åˆæ£€ç´¢ï¼‰
**æ ¸å¿ƒç†å¿µ**ï¼šç»“åˆå‘é‡æ£€ç´¢çš„é€Ÿåº¦ä¼˜åŠ¿å’Œç›®å½•æ ‘ç»“æ„çš„ä¸Šä¸‹æ–‡ä¼˜åŠ¿ã€‚

**æ£€ç´¢æµç¨‹**ï¼š
```
ç”¨æˆ·æŸ¥è¯¢ 
    â†“
1. æŸ¥è¯¢æ–‡æœ¬å‘é‡åŒ–
    â†“
2. å‘é‡æ£€ç´¢ Top-K ç›¸å…³èŠ‚ç‚¹ï¼ˆåŸºäºèŠ‚ç‚¹æ ‡é¢˜+æ‘˜è¦çš„å‘é‡ï¼‰
    â†“
3. æ ¹æ®èŠ‚ç‚¹çš„ start_index/end_index æå–åŸæ–‡
    â†“
4. LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
```

**ä¼˜ç‚¹**ï¼š
- æ£€ç´¢é€Ÿåº¦å¿«ï¼ˆæ¯«ç§’çº§ï¼‰
- æ£€ç´¢é˜¶æ®µä¸æ¶ˆè€— Token
- ä¿ç•™äº†æ–‡æ¡£çš„å±‚çº§ç»“æ„ä¿¡æ¯
- æ¯ä¸ªèŠ‚ç‚¹æœ‰å®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆä¸æ˜¯éšæœºåˆ‡ç‰‡ï¼‰

**å…³é”®å·®å¼‚**ï¼š
- å‘é‡åŒ–çš„æ˜¯èŠ‚ç‚¹æ‘˜è¦è€ŒéåŸæ–‡åˆ‡ç‰‡
- æ£€ç´¢å•ä½æ˜¯ç›®å½•èŠ‚ç‚¹è€Œéå›ºå®šå¤§å°çš„å—
- ä¿ç•™äº†èŠ‚ç‚¹çš„å±‚çº§è·¯å¾„å’Œé¡µç èŒƒå›´ä¿¡æ¯
"""

# æ ‡é¢˜å’ŒåŸç†ä»‹ç»æŒ‰é’®
col_title, col_info = st.columns([6, 1])
with col_title:
    st.title("ğŸŒ² PageIndex + Vector æ™ºèƒ½RAGæ£€ç´¢ç³»ç»Ÿ")
with col_info:
    st.write("")  # å ä½ï¼Œä½¿æŒ‰é’®å‚ç›´å±…ä¸­
    if st.button("â„¹ï¸ åŸç†ä»‹ç»", help="ç‚¹å‡»æŸ¥çœ‹æ£€ç´¢åŸç†"):
        st.session_state.show_principle = True

# åŸç†ä»‹ç»å¼¹çª—
if st.session_state.get("show_principle", False):
    @st.dialog("ğŸ“– PageIndex æ£€ç´¢åŸç†ä»‹ç»", width="large")
    def show_principle_dialog():
        st.markdown(PRINCIPLE_CONTENT)
        if st.button("å…³é—­", type="primary"):
            st.session_state.show_principle = False
            st.rerun()
    show_principle_dialog()

tab1, tab2, tab3 = st.tabs(["ğŸ“„ æ–‡æ¡£å¤„ç†", "ğŸ’¬ æ™ºèƒ½å¯¹è¯", "ğŸ“Š å‘é‡ç´¢å¼•"])

upload_dir = "uploads"
results_dir = "results"
os.makedirs(upload_dir, exist_ok=True)
os.makedirs(results_dir, exist_ok=True)

# é€‰é¡¹å¡ 1: æ–‡æ¡£å¤„ç†
with tab1:
    st.header("å¤„ç†æ–°æ–‡æ¡£")
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ æ–‡ä»¶", 
        type=["pdf", "md", "markdown"], 
        accept_multiple_files=True,
        key="file_uploader"
    )

    # é‡å¤æ–‡ä»¶æ£€æµ‹
    if uploaded_files:
        duplicates = check_duplicate_files(uploaded_files, upload_dir)
        if duplicates:
            st.warning(f"âš ï¸ æ£€æµ‹åˆ°é‡å¤æ–‡ä»¶ï¼ä»¥ä¸‹æ–‡ä»¶å·²å­˜åœ¨äºä¸Šä¼ ç›®å½•ä¸­ï¼š\n\n**{', '.join(duplicates)}**\n\nç»§ç»­å¤„ç†å°†è¦†ç›–åŸæœ‰æ–‡ä»¶ã€‚")

        if st.button("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†"):
            if not api_key:
                st.error("è¯·è¾“å…¥ API å¯†é’¥ï¼")
            else:
                update_api_config(api_key, api_base)
                total_files = len(uploaded_files)
                # æ˜¾ç¤ºæ€»ä½“è¿›åº¦ä¿¡æ¯
                overall_status = st.empty()
                # å•æ–‡æ¡£è¿›åº¦æ¡
                progress_bar = st.progress(0.0)
                status_text = st.empty()
                all_results_container = st.container()
                
                for i, uploaded_file in enumerate(uploaded_files):
                    file_extension = os.path.splitext(uploaded_file.name)[1].lower()
                    overall_status.info(f"ğŸ“ æ€»è¿›åº¦: {i+1}/{total_files} ä¸ªæ–‡ä»¶")
                    status_text.text(f"æ­£åœ¨å¤„ç†: {uploaded_file.name}")
                    # é‡ç½®è¿›åº¦æ¡ä¸º0
                    progress_bar.progress(0.0)
                    
                    try:
                        # é˜¶æ®µ1: ä¿å­˜æ–‡ä»¶ (10%)
                        progress_bar.progress(0.1)
                        file_path = os.path.join(upload_dir, uploaded_file.name)
                        with open(file_path, "wb") as f:
                            f.write(uploaded_file.getvalue())
                        
                        # é˜¶æ®µ2: å¼€å§‹å¤„ç† (20%)
                        progress_bar.progress(0.2)
                        result = None
                        if file_extension == ".pdf":
                            # é˜¶æ®µ3: PDFè§£æä¸­ (40%)
                            progress_bar.progress(0.4)
                            opt = config(
                                model=model_name,
                                toc_check_page_num=toc_check_pages,
                                max_page_num_each_node=max_pages_per_node,
                                max_token_num_each_node=max_tokens_per_node,
                                if_add_node_id="yes",
                                if_add_node_summary="yes",
                                if_add_doc_description=if_add_doc_description,
                                if_add_node_text=if_add_node_text,
                                if_build_vector_index="yes"  # è‡ªåŠ¨æ„å»ºå‘é‡ç´¢å¼•
                            )
                            result = page_index_main(file_path, opt)
                        elif file_extension in [".md", ".markdown"]:
                            # é˜¶æ®µ3: Markdownè§£æä¸­ (40%)
                            progress_bar.progress(0.4)
                            result = asyncio.run(md_to_tree(
                                md_path=file_path,
                                if_thinning=False,
                                if_add_node_summary=True,
                                model=model_name,
                                if_add_doc_description=(if_add_doc_description == "yes"),
                                if_add_node_text=True,  # Markdown æ–‡ä»¶å¼ºåˆ¶ä¿ç•™å®Œæ•´æ–‡æœ¬ä»¥æ”¯æŒæ£€ç´¢
                                if_add_node_id=True,
                                if_build_vector_index=True  # è‡ªåŠ¨æ„å»ºå‘é‡ç´¢å¼•
                            ))
                        
                        # é˜¶æ®µ4: ç”Ÿæˆæ‘˜è¦ä¸­ (70%)
                        progress_bar.progress(0.7)

                        if result:
                            # é˜¶æ®µ5: ä¿å­˜ç»“æœ (90%)
                            progress_bar.progress(0.9)
                            file_base_name = os.path.splitext(uploaded_file.name)[0]
                            result_file_path = os.path.join(results_dir, f"{file_base_name}_structure.json")
                            with open(result_file_path, "w", encoding="utf-8") as f:
                                json.dump(result, f, indent=2, ensure_ascii=False)
                            
                            # é˜¶æ®µ6: å®Œæˆ (100%)
                            progress_bar.progress(1.0)
                            with all_results_container:
                                with st.expander(f"âœ… {uploaded_file.name} å¤„ç†æˆåŠŸ", expanded=False):
                                    st.info(f"JSON å·²è‡ªåŠ¨ä¿å­˜è‡³: {result_file_path}")
                                    st.json(result)
                    except Exception as e:
                        progress_bar.progress(1.0)
                        with all_results_container:
                            st.error(f"âŒ {uploaded_file.name} å¤„ç†å‡ºé”™: {str(e)}")
                
                overall_status.success(f"ğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼å…±å¤„ç† {total_files} ä¸ªæ–‡ä»¶")
                status_text.empty()
                progress_bar.empty()
                st.balloons()

    # æ–‡ä»¶è¯¦ç»†æ¸…å•
    st.markdown("---")
    files_info = get_uploaded_files_info(upload_dir)
    
    # æ ‡é¢˜å’Œåˆ é™¤æŒ‰é’®åœ¨åŒä¸€è¡Œ
    col_title, col_btn = st.columns([4, 1])
    with col_title:
        st.subheader("ğŸ“‹ å·²å¤„ç†æ–‡ä»¶æ¸…å•")
    
    if files_info:
        file_names = [f['æ–‡ä»¶å'] for f in files_info]
        
        # åˆå§‹åŒ–é€‰ä¸­çŠ¶æ€
        if "selected_files" not in st.session_state:
            st.session_state.selected_files = {name: False for name in file_names}
        
        # åŒæ­¥æ–°æ–‡ä»¶åˆ°é€‰ä¸­çŠ¶æ€
        for name in file_names:
            if name not in st.session_state.selected_files:
                st.session_state.selected_files[name] = False
        
        with col_btn:
            if st.button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­", type="secondary"):
                deleted_files = []
                vector_index = get_vector_index()
                
                for filename, selected in st.session_state.selected_files.items():
                    if selected:
                        # åˆ é™¤åŸå§‹æ–‡ä»¶
                        file_path = os.path.join(upload_dir, filename)
                        if os.path.exists(file_path):
                            os.remove(file_path)
                        
                        # åˆ é™¤å¯¹åº”çš„ç´¢å¼• JSON æ–‡ä»¶
                        file_base_name = os.path.splitext(filename)[0]
                        json_path = os.path.join(results_dir, f"{file_base_name}_structure.json")
                        if os.path.exists(json_path):
                            os.remove(json_path)
                        
                        # åˆ é™¤å‘é‡ç´¢å¼•
                        try:
                            vector_index.delete_document(file_base_name)
                        except Exception as e:
                            st.warning(f"åˆ é™¤ {file_base_name} çš„å‘é‡ç´¢å¼•å¤±è´¥: {e}")
                        
                        deleted_files.append(filename)
                
                if deleted_files:
                    st.success(f"å·²åˆ é™¤ {len(deleted_files)} ä¸ªæ–‡ä»¶")
                    st.session_state.selected_files = {}
                    st.rerun()
                else:
                    st.warning("è¯·å…ˆé€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶")
        
        # ä½¿ç”¨ data_editor å®ç°ç´§å‡‘çš„å¯é€‰æ‹©è¡¨æ ¼ï¼Œè®¾ç½®å›ºå®šé«˜åº¦å®ç°æ»šåŠ¨
        df = pd.DataFrame(files_info)
        df.insert(0, 'é€‰æ‹©', False)
        
        # å°†åºå·è½¬ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿å±…ä¸­æ˜¾ç¤º
        df['åºå·'] = df['åºå·'].astype(str)
        
        # è®¡ç®—è¡¨æ ¼é«˜åº¦ï¼šæ¯è¡Œçº¦35pxï¼Œè¡¨å¤´çº¦35pxï¼Œæœ€å¤§æ˜¾ç¤º10è¡Œ
        table_height = min(len(files_info) * 35 + 35, 400)
        
        edited_df = st.data_editor(
            df,
            column_config={
                "é€‰æ‹©": st.column_config.CheckboxColumn(
                    "é€‰æ‹©",
                    help="é€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶",
                    default=False,
                ),
                "åºå·": st.column_config.TextColumn("åºå·", width="small"),
                "æ–‡ä»¶å": st.column_config.TextColumn("æ–‡ä»¶å", width="medium"),
                "ä¸Šä¼ æ—¶é—´": st.column_config.TextColumn("ä¸Šä¼ æ—¶é—´", width="medium"),
                "æ–‡ä»¶å¤§å°": st.column_config.TextColumn("å¤§å°", width="small"),
                "æ–‡ä»¶ç±»å‹": st.column_config.TextColumn("ç±»å‹", width="small"),
            },
            disabled=["åºå·", "æ–‡ä»¶å", "ä¸Šä¼ æ—¶é—´", "æ–‡ä»¶å¤§å°", "æ–‡ä»¶ç±»å‹"],
            hide_index=True,
            use_container_width=True,
            height=table_height,
            key="file_table"
        )
        
        # æ›´æ–°é€‰ä¸­çŠ¶æ€
        for idx, row in edited_df.iterrows():
            st.session_state.selected_files[row['æ–‡ä»¶å']] = row['é€‰æ‹©']
        
        st.caption(f"å…± {len(files_info)} ä¸ªæ–‡ä»¶")
    else:
        with col_btn:
            st.empty()
        st.info("æš‚æ— å·²ä¸Šä¼ çš„æ–‡ä»¶ã€‚è¯·ä¸Šä¼ æ–‡ä»¶è¿›è¡Œå¤„ç†ã€‚")

# é€‰é¡¹å¡ 2: æ™ºèƒ½å¯¹è¯ (RAG) - ä½¿ç”¨å‘é‡æ£€ç´¢
with tab2:
    st.header("è·¨æ–‡æ¡£æ™ºèƒ½å¯¹è¯")
    
    # æ£€æŸ¥å‘é‡ç´¢å¼•çŠ¶æ€
    try:
        vector_index = get_vector_index()
        stats = vector_index.get_stats()
        
        if stats["total_nodes"] == 0:
            st.warning("å‘é‡ç´¢å¼•ä¸ºç©ºã€‚è¯·å…ˆåœ¨ã€Œæ–‡æ¡£å¤„ç†ã€é€‰é¡¹å¡ä¸­å¤„ç†æ–‡ä»¶ï¼Œæˆ–åœ¨ã€Œå‘é‡ç´¢å¼•ã€é€‰é¡¹å¡ä¸­é‡å»ºç´¢å¼•ã€‚")
        else:
            st.success(f"âœ… å‘é‡ç´¢å¼•å°±ç»ªï¼š{stats['total_documents']} ä¸ªæ–‡æ¡£ï¼Œ{stats['total_nodes']} ä¸ªèŠ‚ç‚¹")
    except Exception as e:
        st.error(f"å‘é‡ç´¢å¼•åˆå§‹åŒ–å¤±è´¥: {e}")
        st.info("è¯·æ£€æŸ¥ Embedding æ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®ã€‚")

    # åˆå§‹åŒ–èŠå¤©å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # åˆ›å»ºèŠå¤©æ¶ˆæ¯å®¹å™¨
    chat_container = st.container()
    
    # èŠå¤©è¾“å…¥æ”¾åœ¨å®¹å™¨å¤–é¢ï¼ˆåº•éƒ¨ï¼‰
    query = st.chat_input("å‘æ•´ä¸ªæ–‡æ¡£åº“æé—®...")
    
    # åœ¨å®¹å™¨å†…æ˜¾ç¤ºèŠå¤©æ¶ˆæ¯
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                if "thinking" in message and message["thinking"]:
                    with st.expander("æ¨ç†æ£€ç´¢è¿‡ç¨‹"):
                        st.markdown(message["thinking"])
                if "nodes" in message and message["nodes"]:
                    with st.expander("å‚è€ƒæ¥æº"):
                        for node_info in message["nodes"]:
                            st.write(node_info)

    # å¤„ç†ç”¨æˆ·è¾“å…¥
    if query:
        if not api_key:
            st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ é…ç½® API å¯†é’¥")
        else:
            update_api_config(api_key, api_base)
            st.session_state.messages.append({"role": "user", "content": query})
            with st.chat_message("user"):
                st.markdown(query)

            with st.chat_message("assistant"):
                with st.status("æ­£åœ¨è¿›è¡Œå‘é‡æ£€ç´¢...", expanded=True) as status:
                    thinking_parts = []
                    all_reference_nodes = []
                    all_relevant_text = ""
                    
                    # 1. å‘é‡æ£€ç´¢ï¼ˆæ¯«ç§’çº§ï¼‰
                    st.write("1. å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢...")
                    try:
                        search_results = search_documents(query, top_k=vector_top_k)
                        thinking_parts.append(f"å‘é‡æ£€ç´¢è¿”å› {len(search_results)} ä¸ªç›¸å…³èŠ‚ç‚¹")
                    except Exception as e:
                        st.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {e}")
                        search_results = []
                    
                    if search_results:
                        # æŒ‰æ–‡æ¡£åˆ†ç»„
                        doc_results = {}
                        for result in search_results:
                            doc_name = result["doc_name"]
                            if doc_name not in doc_results:
                                doc_results[doc_name] = []
                            doc_results[doc_name].append(result)
                        
                        st.write(f"æ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³èŠ‚ç‚¹ï¼Œæ¥è‡ª {len(doc_results)} ä¸ªæ–‡æ¡£")
                        
                        # 2. å†…å®¹æå–
                        st.write("2. æå–ç›¸å…³å†…å®¹...")
                        for doc_name, results in doc_results.items():
                            doc_data = load_document_structure(doc_name, results_dir)
                            if not doc_data:
                                thinking_parts.append(f"[{doc_name}] æœªæ‰¾åˆ°ç»“æ„æ–‡ä»¶")
                                continue
                            
                            node_map = get_node_mapping(doc_data.get("structure", []))
                            
                            for result in results:
                                node_id = result["node_id"]
                                title = result["title"]
                                score = result.get("score", 0)
                                
                                all_reference_nodes.append(f"[{doc_name}] {title} (ç›¸ä¼¼åº¦: {score:.3f})")
                                
                                node = node_map.get(node_id)
                                if node and node.get("text"):
                                    all_relevant_text += f"\n--- æ–‡æ¡£: {doc_name}, ç« èŠ‚: {title} ---\n{node['text']}\n"
                                elif result.get("summary"):
                                    all_relevant_text += f"\n--- æ–‡æ¡£: {doc_name}, ç« èŠ‚: {title} (æ‘˜è¦) ---\n{result['summary']}\n"
                        
                        st.write("3. ç”Ÿæˆå›ç­”...")
                        status.update(label="å‘é‡æ£€ç´¢å®Œæˆ", state="complete", expanded=False)
                    else:
                        status.update(label="æœªæ‰¾åˆ°ç›¸å…³å†…å®¹", state="error", expanded=False)

                # 3. ç”Ÿæˆç­”æ¡ˆ
                if not all_relevant_text.strip():
                    full_answer = "æŠ±æ­‰ï¼Œæœªèƒ½ä»æ–‡æ¡£åº“ä¸­æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚è¯·å°è¯•æ¢ä¸€ç§æ–¹å¼æé—®ï¼Œæˆ–ç¡®ä¿ç›¸å…³æ–‡æ¡£å·²è¢«å¤„ç†ã€‚"
                else:
                    answer_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ã€‚ä½ æœ‰æ¥è‡ªå¤šä¸ªæ¥æºçš„æ–‡æ¡£ç‰‡æ®µã€‚
æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœæ¥æºæœ‰å†²çªçš„ä¿¡æ¯ï¼Œè¯·æåŠã€‚
åœ¨å›ç­”ä¸­å§‹ç»ˆå¼•ç”¨æ–‡æ¡£åç§°ã€‚

é—®é¢˜: {query}

ä¸Šä¸‹æ–‡:
{all_relevant_text[:15000]}

åŠ©æ‰‹:"""
                    try:
                        full_answer = ChatGPT_API(model=model_name, prompt=answer_prompt)
                    except Exception as e:
                        full_answer = f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}"
                
                st.markdown(full_answer)
                if all_reference_nodes:
                    with st.expander("å‚è€ƒæ¥æº"):
                        for node_info in all_reference_nodes:
                            st.write(node_info)
                
                # ä¿å­˜å†å²è®°å½•
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": full_answer,
                    "thinking": "\n".join(thinking_parts),
                    "nodes": all_reference_nodes
                })

# é€‰é¡¹å¡ 3: å‘é‡ç´¢å¼•ç®¡ç†
with tab3:
    st.header("å‘é‡ç´¢å¼•ç®¡ç†")
    
    # æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡
    try:
        vector_index = get_vector_index()
        stats = vector_index.get_stats()
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æ€»èŠ‚ç‚¹æ•°", stats["total_nodes"])
        with col2:
            st.metric("å·²ç´¢å¼•æ–‡æ¡£æ•°", stats["total_documents"])
        with col3:
            st.metric("ç´¢å¼•çŠ¶æ€", "æ­£å¸¸" if stats["total_nodes"] > 0 else "ç©º")
        
        if stats["documents"]:
            st.subheader("å·²ç´¢å¼•æ–‡æ¡£åˆ—è¡¨")
            for doc in stats["documents"]:
                node_count = vector_index.get_document_node_count(doc)
                st.write(f"- **{doc}**: {node_count} ä¸ªèŠ‚ç‚¹")
    except Exception as e:
        st.error(f"è·å–ç´¢å¼•ç»Ÿè®¡å¤±è´¥: {e}")
    
    st.markdown("---")
    
    # ç´¢å¼•æ“ä½œ
    col_rebuild, col_clear = st.columns(2)
    
    with col_rebuild:
        if st.button("ğŸ”„ é‡å»ºæ‰€æœ‰ç´¢å¼•", type="primary"):
            with st.spinner("æ­£åœ¨é‡å»ºç´¢å¼•..."):
                try:
                    vector_index = get_vector_index()
                    
                    structure_files = [f for f in os.listdir(results_dir) if f.endswith("_structure.json")]
                    
                    if not structure_files:
                        st.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç»“æ„æ–‡ä»¶")
                    else:
                        progress = st.progress(0)
                        rebuilt_count = 0
                        
                        for i, filename in enumerate(structure_files):
                            try:
                                filepath = os.path.join(results_dir, filename)
                                with open(filepath, "r", encoding="utf-8") as f:
                                    data = json.load(f)
                                
                                doc_name = data.get("doc_name", filename.replace("_structure.json", ""))
                                doc_description = data.get("doc_description", "")
                                structure = data.get("structure", [])
                                
                                node_count = vector_index.add_document(doc_name, structure, doc_description)
                                rebuilt_count += 1
                                
                            except Exception as e:
                                st.warning(f"é‡å»º {filename} å¤±è´¥: {e}")
                            
                            progress.progress((i + 1) / len(structure_files))
                        
                        st.success(f"âœ… ç´¢å¼•é‡å»ºå®Œæˆï¼å…±å¤„ç† {rebuilt_count} ä¸ªæ–‡æ¡£")
                        st.rerun()
                        
                except Exception as e:
                    st.error(f"é‡å»ºç´¢å¼•å¤±è´¥: {e}")
    
    with col_clear:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰ç´¢å¼•", type="secondary"):
            try:
                vector_index = get_vector_index()
                docs = vector_index.get_all_documents()
                
                for doc in docs:
                    vector_index.delete_document(doc)
                
                st.success(f"âœ… å·²æ¸…ç©º {len(docs)} ä¸ªæ–‡æ¡£çš„ç´¢å¼•")
                st.rerun()
            except Exception as e:
                st.error(f"æ¸…ç©ºç´¢å¼•å¤±è´¥: {e}")
    
    # Embedding æ¨¡å‹é…ç½®ä¿¡æ¯
    st.markdown("---")
    st.subheader("Embedding æ¨¡å‹é…ç½®")
    
    embedding_model_name = os.getenv("EMBEDDING_MODEL_NAME", "bge-m3:latest")
    embedding_api_url = os.getenv("EMBEDDING_MODEL_API_URL", "http://10.20.2.135:11434")
    
    st.code(f"""
EMBEDDING_MODEL_NAME={embedding_model_name}
EMBEDDING_MODEL_API_URL={embedding_api_url}
EMBEDDING_MODEL_TYPE=ollama
    """, language="bash")
    
    # æµ‹è¯• Embedding è¿æ¥
    if st.button("ğŸ”— æµ‹è¯• Embedding è¿æ¥"):
        with st.spinner("æ­£åœ¨æµ‹è¯•è¿æ¥..."):
            try:
                from pageindex.vector_index import OllamaEmbedding
                embedding_model = OllamaEmbedding()
                test_embedding = embedding_model.embed("æµ‹è¯•æ–‡æœ¬")
                st.success(f"âœ… è¿æ¥æˆåŠŸï¼Embedding ç»´åº¦: {len(test_embedding)}")
            except Exception as e:
                st.error(f"âŒ è¿æ¥å¤±è´¥: {e}")

st.markdown("---")
st.caption("ç”± PageIndex æ¡†æ¶é©±åŠ¨ - æ··åˆå‘é‡æ£€ç´¢ RAG")
